# -*- coding: utf-8 -*-
"""Bankrupty_Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/171iFtoPexqWUA1ExYzZ7sRhNiQtems3K
"""

# Import necessary libraries

import numpy as np
import pandas as pd

from imblearn.pipeline import Pipeline
from imblearn.ensemble import BalancedRandomForestClassifier

from sklearn.base import BaseEstimator,ClassifierMixin,clone
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split,GridSearchCV,StratifiedKFold
from sklearn.metrics import classification_report,recall_score,accuracy_score,confusion_matrix,make_scorer,f1_score

import matplotlib.pyplot as plt
import seaborn as sns
import joblib

import warnings
warnings.filterwarnings('ignore')

# Load the bankruptcy csv file

df = pd.read_csv('/Users/moepyesone/Desktop/ITCamp/data.csv')
df.head()

# Check how many rows and columns

df.shape

"""We can see that there are 6819 rows and 96 columns"""

# Remove multiple spaces between words and trim leading and trailing spaces

df.columns = df.columns.str.replace(r'\s+',' ',regex=True).str.strip()

# Check null values and column data types

df.info()

"""We can see that there are no NULL values. So we don't need to preprocess NULL values"""

# Check statistics of columns

df.describe()

# Split features and target

x = df.drop('Bankrupt?',axis=1)
y = df['Bankrupt?']

# Check the counts of the target class

y.value_counts()

"""We can see that the target class is extremely imbalanced. There are only 220 bankrupt cases and 6599 not bankrupt cases"""

# Split training and test sets

x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,stratify=y,random_state=42)

# Define a custom Threshold Classifier for prediction

class ThresholdClassifier(BaseEstimator,ClassifierMixin):

  # The object can be called by passing in the classifier and the threshold for prediction

  def __init__(self,base_estimator,threshold):
    self.base_estimator=base_estimator
    self.threshold=threshold

  ''' The object has a fit method. For grid search, it will clone the classifier, trains on the data, and return
  the trained model to the grid search. So the grid search will receive a different model for each hyperparameter
  combination'''

  def fit(self,x,y,**fit_params):
    self.estimator = clone(self.base_estimator)
    self.estimator.fit(x,y,**fit_params)
    return self

  # The object has a predict method. It will use the custom threshold to predict the target

  def predict(self,x):
    proba = self.estimator.predict_proba(x)[:,1]
    return (proba >= self.threshold).astype(int)

  # The object also has a predict_proba method. It will return the original probabilities without the custom threshold

  def predict_proba(self,x):
    return self.estimator.predict_proba(x)

"""Since the target class is the minority, we cannot use the default sklearn prediction threshold, which is 0.5. We need to give more attention and sensitivity to the minority class.

This custom ThresholdClassifier object takes a classification model, trains on the data and make predictions using the custom threshold that is passed in.
"""

# Create a function for a grid search pipeline

def grid_search(threshold):

  # Create a RandomForestClassifier. random_state = 42 for reproducibility and n_jobs = -1 to speed up training

  rf = BalancedRandomForestClassifier(
    random_state=42,
    n_jobs=-1
  )

  # Create the ThresholdClassifier object and pass in the RandomForestClassifier and custom threshold

  classifier = ThresholdClassifier(base_estimator=rf,threshold=threshold)

  # Create a pipeline with SimpleImputer in case the dataset gets NULL values in the future

  pipeline = Pipeline([
      ('imputer',SimpleImputer(strategy='median')),
      ('classifier',classifier)
  ])

  # Create a param grid of RandomForest hyperparameters for the grid search to test

  param_grid = {
    'classifier__base_estimator__n_estimators': [600,800,1000,1200],
    'classifier__base_estimator__max_depth': [None,20,40],
    'classifier__base_estimator__min_samples_leaf':[1,2,3],
    'classifier__base_estimator__criterion':['entropy']
  }

  ''' Use StratifiedKFold with 5 splits for cross validation. This will enable grid search to properly test
      each hyperparameter combination'''

  cv = StratifiedKFold(n_splits=5,random_state=42,shuffle=True)

  # Create a grid search pipeline with classifer and param grid

  grid_search = GridSearchCV(
      estimator=pipeline,
      param_grid=param_grid,
      cv=cv,
      scoring='balanced_accuracy',
      n_jobs=-1,
      verbose=2
  )

  # Returns the grid search

  return grid_search

# Create a function for evaluation metrics

def results(best_model,x_test):

  # Predict the test set with the best model

  y_pred = best_model.predict(x_test)

  # Print the classification report consisting of Recall, F1, Precision and Model Accuracy

  print("\nClassification report:\n\n", classification_report(y_test, y_pred))

  # Visualise with Confusion Matrix so we can see how many true and false predictions

  conf_matrix = confusion_matrix(y_test,y_pred)
  plt.figure()
  sns.heatmap(conf_matrix,annot=True,fmt='d',cmap='Blues')
  plt.title('Bankruptcy Prediction Confusion Matrix')
  plt.xlabel('Predicted')
  plt.ylabel('Actual')

  plt.tight_layout()
  plt.show()

"""For Classification Report, we should focus on the minority class of '1', which means a company goes bankrupt. This model aims to predict if a company has a chance of going bankrupt so relevant parties can take necessary steps to prevent it.

Missing a bankrupt case is much more costly and troublesome than falsely identifying a company as bankrupt. So the model needs to achieve a high recall score for the minority class '1'. That means the model can accurately predict companies that might go bankrupt.

However, the model shouldn't have too many false positives such as predicting too many companies as bankrupt. So the model needs to balance the minority recall and overall model accuracy.

random_state = 42 is used everywhere to achieve reproducible results.
"""

# Build the first grid search with custom threshold of 0.1

grid_search_1 = grid_search(0.1)

grid_search_1.fit(x_train,y_train)

# Choose the best performing model from the grid search

best_model_1 = grid_search_1.best_estimator_

# Check the results of the first best model

results(grid_search_1,x_test)

"""The model achieved 100% recall for the minority class. It means the model predicted every single company that went bankrupt. So this model performs well for predicting companies that might go bankrupt.

However, the model has an overall accuracy score of 60%. The model predicted 539 non bankrupt companies as bankrupt. So the model has quite a large amount of false positives.
"""

# Check the 40 most important features according to the best model.

feature_importances = best_model_1.named_steps['classifier'].estimator.feature_importances_
feature_names = x_train.columns
importance_df = pd.Series(feature_importances,index=feature_names).sort_values(ascending=False)
top_features = importance_df.head(40).index.tolist()

x_top = x[top_features]

# Split the training and test set with the top 40 features

x_train_top,x_test_top,y_train_top,y_test_top = train_test_split(x_top,y,test_size=0.2,stratify=y,random_state=42)

"""The dataset has 96 coloumns. Since there are many columns in the dataset, they might create noise and store unrelevant, unnecessary data. So we shall do feature selection to improve the model accuracy.

The feature importances can be obtained from the best model of grid search. Then the top 40 most important features are chosen. The traing and test splits are filtered with the top 40 features
"""

# Build the second grid search

grid_search_2 = grid_search(0.15)

grid_search_2.fit(x_train_top,y_train_top)

# Choose the best performing model from the second grid search

best_model_2 = grid_search_2.best_estimator_

# Check the results of the second best model

results(best_model_2,x_test_top)

"""The accuracy of the model increased from 60% to 70% after testing on the top 40 features. The false positives dropped from 539 to 407. But the minority recall is still 100%. 100% recall and 70% accuracy is an acceptable tradeoff."""

# Export the second best model

joblib.dump(best_model_2,'/Users/moepyesone/Desktop/ITCamp/Bankruptcy_Prediction_Model.pkl')

"""The trained model pipeline is exported via joblib and stored in 'Bankruptcy_Prediction_Model.pkl'. The purpose is to enable different collaborators to make predictions on the dataset without training the model again."""

# Load the exported model

model = joblib.load('/Users/moepyesone/Desktop/ITCamp/Bankruptcy_Prediction_Model.pkl')

# Test the loaded model

results(model,x_test_top)

"""The exported model is loaded and stored in 'model'. And the predictions are made on the testing set. We can see that the loaded model achieves the same results as the exported model. So this model is fully reusable."""